#+title: Notes of the second day

* point-to-point
Solve deadlock: manually control the send/recv order on each process
#+begin_src cpp :eval never
if (rank % 2 == 0)
{
  MPI_Send(buffer_send, dst);
  MPI_Recv(buffer_recv, src);
}
else
{
  MPI_Recv(buffer_recv, src);
  MPI_Send(buffer_send, dst);
}
#+end_src

flush stdout before barrier to make output consistent with the code.
#+begin_src cpp :eval never
fflush(stdout);
#+end_src

MPI status is a struct in C++
#+begin_src cpp :eval never
MPI_Status :: ms
#+end_src
In Fortran, MPI status is a derived type ~mpi_status~, or an integer array of size ~MPI_STATUS_SIZE~
#+begin_src f90 :eval never
type(MPI_STATUS) :: ms
integer :: ms(MPI_STATUS_SIZE)
#+end_src

Qs
- ~MPI_DOUBLE~ vs ~MPI_DOUBLE_PRECISION~ in Fortran
- why no status in bcast?

** Exercise: message-exchange
msgsize increased to 1000000, larger than arraysize 100000, gives Segmentation fault.

Send 100 int
#+begin_src
Rank 0 received 100 elements, first 1
Time Send/Recv on rank 0: 0.000034
Rank 1 received 100 elements, first 0
Time Send/Recv on rank 1: 0.000195
#+end_src

Send 100000 int
#+begin_src
Rank 1 received 100000 elements, first 0
Time Send/Recv on rank 1: 0.000459
Rank 0 received 100000 elements, first 1
Time Send/Recv on rank 0: 0.000472
#+end_src

** Exercise: message-chain
:PROPERTIES:
:CUSTOM_ID: exercise-message-chain
:END:
链式传输. 4 processes
#+begin_src
  Sender:   3     Sent elements: 10000000. Tag:   4. Receiver:  -1
Receiver:   3 First element:   2
  Sender:   2     Sent elements: 10000000. Tag:   3. Receiver:   3
Receiver:   2 First element:   1
  Sender:   1     Sent elements: 10000000. Tag:   2. Receiver:   2
Receiver:   1 First element:   0
  Sender:   0     Sent elements: 10000000. Tag:   1. Receiver:   1
Receiver:   0 First element:  -1
Time elapsed in rank  0: 0.050
Time elapsed in rank  1: 0.050
Time elapsed in rank  2: 0.034
Time elapsed in rank  3: 0.017
#+end_src

Rank 3 只接受数据，所以只有一次传输 overhead.
Rank 2 先发送给 3, 然后接受 1，两次传输。
Rank 1 发送给 2, 但要等 Rank 2 到达 Recv, 所以需要等待一次传输再进行 Send/Recv, 总共间隔 3 次传输时间。
Rank 0 不需要再接收其他数据，传输完成。

8 processes
#+begin_src
Time elapsed in rank  0: 0.118
Time elapsed in rank  1: 0.118
Time elapsed in rank  2: 0.102
Time elapsed in rank  3: 0.085
Time elapsed in rank  4: 0.068
Time elapsed in rank  5: 0.051
Time elapsed in rank  6: 0.035
Time elapsed in rank  7: 0.018
#+end_src

总传输时间 \(t = (n - 1) * t_1\)

* Collective
** Exercise: broadcast-scatter
Broadcast (4 procs)
#+begin_src
Task  0:  0  1  2  3  4  5  6  7  8  9 10 11
Task  1: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
Task  2: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
Task  3: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1

Task  0:  0  1  2  3  4  5  6  7  8  9 10 11
Task  1:  0  1  2  3  4  5  6  7  8  9 10 11
Task  2:  0  1  2  3  4  5  6  7  8  9 10 11
Task  3:  0  1  2  3  4  5  6  7  8  9 10 11
#+end_src

Scatter (4 procs)
#+begin_src
Task  0:  0  1  2  3  4  5  6  7  8  9 10 11
Task  1: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
Task  2: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
Task  3: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1

Task  0:  0  1  2 -1 -1 -1 -1 -1 -1 -1 -1 -1
Task  1:  3  4  5 -1 -1 -1 -1 -1 -1 -1 -1 -1
Task  2:  6  7  8 -1 -1 -1 -1 -1 -1 -1 -1 -1
Task  3:  9 10 11 -1 -1 -1 -1 -1 -1 -1 -1 -1
#+end_src

* Debugger
[[https://deadlockempire.github.io/][The Deadlock Empire]]

Arm DDT
#+begin_src shell :eval never
module load ARMForge
export SLURM_OVERLAP=1
salloc -A project_465000536 --nodes=1 --ntasks-per-node=2 --time=00:30:00 --partition=debug
ddt srun ./buggy
#+end_src

Try salloc and srun for debugging (not necessary interactive, if allowed)
#+begin_src shell :eval never
salloc
srun --pty bash
#+end_src

salloc session timeout 之后，可能还在 salloc 的 subshell 里面，
环境变量 ~SLURM_JOB_ID~ 被固定成 salloc 分配的 ID.
此时如果再次 srun, 会报错
#+begin_src
srun: error: Unable to confirm allocation for job <job-id>: Invalid job id specified
srun: Check SLURM_JOB_ID environment variable. Expired or invalid job <job-id>
#+end_src
解决方案就是 exit 或者 ctrl-D 退出 subshell.

~--label~ 会在并行输出的行首加上进程号。

* On the heat-equation project
