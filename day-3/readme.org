#+title: Notes of third day

* Communicator (user define)
#+begin_src cpp :eval never
MPI_Comm_split(mother_comm, color, key, &sub_comm);
MPI_Comm_free(&sub_comm);
#+end_src

* Non-blocking communication
** APIs
#+begin_src cpp :eval never
int MPI_Waitany(int count, MPI_Request array_of_requests[],
    int *index, MPI_Status *status);
int MPI_Waitsome(int incount, MPI_Request array_of_requests[],
                int *outcount, int array_of_indices[],
                MPI_Status array_of_statuses[]);

int MPI_Testany(int count, MPI_Request array_of_requests[], int *indx,
               int *flag, MPI_Status *status);
int MPI_Testsome(int incount, MPI_Request array_of_requests[], int *outcount,
                int array_of_indices[], MPI_Status array_of_statuses[]);

int MPI_Iprobe(int source, int tag, MPI_Comm comm, int *flag, MPI_Status *status);
#+end_src

** Exercise: message-chain-nonblocking
See message-chain exercise in [[../day-1/readme.org][day-1 notes]].

非阻塞式的链式传输. 4 processes
#+begin_src
Sender: 0. Sent elements: 10000000. Tag: 1. Receiver: 1
Receiver: 0. first element -1.
Sender: 1. Sent elements: 10000000. Tag: 2. Receiver: 2
Receiver: 1. first element 0.
Sender: 2. Sent elements: 10000000. Tag: 3. Receiver: 3
Receiver: 2. first element 1.
Sender: 3. Sent elements: 10000000. Tag: 4. Receiver: -1
Receiver: 3. first element 2.
Time elapsed in rank  0:  0.000
Time elapsed in rank  1:  0.021
Time elapsed in rank  2:  0.026
Time elapsed in rank  3:  0.027
#+end_src

8 processes
#+begin_src
Sender: 0. Sent elements: 10000000. Tag: 1. Receiver: 1
Receiver: 0. first element -1.
Sender: 1. Sent elements: 10000000. Tag: 2. Receiver: 2
Receiver: 1. first element 0.
Sender: 2. Sent elements: 10000000. Tag: 3. Receiver: 3
Receiver: 2. first element 1.
Sender: 4. Sent elements: 10000000. Tag: 5. Receiver: 5
Receiver: 4. first element 3.
Sender: 5. Sent elements: 10000000. Tag: 6. Receiver: 6
Receiver: 5. first element 4.
Sender: 6. Sent elements: 10000000. Tag: 7. Receiver: 7
Receiver: 6. first element 5.
Sender: 7. Sent elements: 10000000. Tag: 8. Receiver: -1
Receiver: 7. first element 6.
Sender: 3. Sent elements: 10000000. Tag: 4. Receiver: 4
Receiver: 3. first element 2.
Time elapsed in rank  0:  0.000
Time elapsed in rank  1:  0.024
Time elapsed in rank  2:  0.026
Time elapsed in rank  3:  0.027
Time elapsed in rank  4:  0.026
Time elapsed in rank  5:  0.026
Time elapsed in rank  6:  0.026
Time elapsed in rank  7:  0.026
#+end_src

上面两个测试里 rank 0 是 0.000 s, 因为我用 wait 等待 Irecv 的 request,
而 rank 0 的 Irecv 是 dummy call, 所以立即通过了 wait, 计时器结束。
如果使用 Waitall 等待 Isend 和 Irecv 的 requests, 那么 rank 0 的时间和其他是相近的。
比如 8 进程
#+begin_src
Time elapsed in rank  0:  0.023
Time elapsed in rank  1:  0.023
Time elapsed in rank  2:  0.023
Time elapsed in rank  3:  0.023
Time elapsed in rank  4:  0.023
Time elapsed in rank  5:  0.025
Time elapsed in rank  6:  0.026
Time elapsed in rank  7:  0.026
#+end_src

Rank 0 time
| Nprocs | block | non-block | persist | Sendrecv |
|--------+-------+-----------+---------+----------|
|      2 | 0.016 |     0.015 |   0.016 |    0.016 |
|      4 | 0.047 |     0.018 |   0.019 |    0.017 |
|      8 | 0.114 |     0.023 |   0.021 |    0.021 |
|     12 | 0.173 |     0.026 |   0.025 |          |
|     16 | 0.239 |     0.034 |   0.034 |          |
|     20 |       |     0.034 |         |          |
|     24 |       |     0.035 |         |          |
|     32 |       |     0.036 |         |          |
|     40 |       |     0.041 |         |          |
|     48 |       |     0.034 |         |          |
|     56 |       |     0.037 |         |          |
|     64 |       |     0.042 |         |          |

* Persistent communication
Another way to do non-blocking communication, but allows MPI library to optimize
for you.

* Cartesian grid
API
#+begin_src cpp :eval never
const int ndims = 3;
MPI_Comm comm3d;
int dims[3] = {2, 3, 4};
int period[3] = {false, false, true};
MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, period, false, &comm3d);

// neighbor ranks (in linear)
int neighbors[2 * ndims];
for (int i = 0; i < ndims; i++)
  MPI_Cart_shift(comm3d, i, 1, neighbors[i*ndims], neighbors[i*ndims + 1]);
#+end_src

* User-defined datatype
Extent check in type vector or type struct.

~MPI_Type_indexed~ 可以看成是 ~MPI_Type_vectorv~.

It is not necessary to free a datatype which has not bee committed.

** Exercise: datatypes-extent
Source array
#+begin_src
11 12 13 14 15 16
21 22 23 24 25 26
31 32 33 34 35 36
41 42 43 44 45 46
51 52 53 54 55 56
61 62 63 64 65 66
71 72 73 74 75 76
81 82 83 84 85 86
#+end_src

Passing only first row
#+begin_src
11 12 13 14 15 16
 0  0  0  0  0  0
 0  0  0  0  0  0
 0  0  0  0  0  0
 0  0  0  0  0  0
 0  0  0  0  0  0
 0  0  0  0  0  0
 0  0  0  0  0  0
#+end_src

Naively passing two rows
#+begin_src
11 12 13 14 15 16
 0  0  0  0  0 26
 0  0  0  0  0  0
 0  0  0  0  0  0
 0  0  0  0  0  0
 0  0  0  0  0  0
 0  0  0  0  0  0
 0  0  0  0  0  0
#+end_src

Extent should be set to only 1 integer size.

~sizeof~ belongs to GNU extension.
For Fortran 2008 and above, there is ~storage_size~ to get the /bits/ of an object.
#+begin_src f90 :eval never
integer :: i
print*, storage_size(i) / 8 == sizeof(i)
#+end_src

** Exercise: datatypes-struct
Using struct
#+begin_src
Time:  0,  3.61374739999973841E-5
Check: 0 Xe 6.170943379E-3,  0.52413851,  0.150425255
Time:  1,  3.61386342000059863E-5
Check: 1 Xe 6.170943379E-3,  0.52413851,  0.150425255
#+end_src

Using bytes
#+begin_src
Time:  0,  7.44354399967051045E-7
Check: 0 Xe 6.170943379E-3,  0.52413851,  0.150425255
Time:  1,  7.46007499992629151E-7
Check: 1 Xe 6.170943379E-3,  0.52413851,  0.150425255
#+end_src

In this case, communicating using ~MPI_Byte~ is about 50 times faster than struct.

* Lightning talk
Mats: Biophysics

Arun: machine learning

Irandoost: dimension reduction

Miro: SYCLomatic

Aikaterini: cosmology

Melina: Neuroscience, brain imaging

Vadim: Enterprise security

Daulet: Human airways

Marek: sea-ice ridging

Kwabena: constrain reservoir evolution (Rheology)

Simo: CS, Linux boot

Ethan: cosmological phase transition

Moris: low-scaling RI

George: solar physics, MHD equations

Saska: CPU level, instruction

David: Vehicle routing problem

Zhennan: Shandong, cellulose nanocrystal 纤维素纳米晶

Qing-Long: core level

Ilkka: MRI, image analysis

Leo: near-Earth plasma (Vlasiator code)

Dandan: Cryo-Electron Microscopy, image processing, CryoSPARC and AlphaFold

Hongding: Luke, Genomic selection

Ahmed: peatland 泥炭地 forestry management scenarios

Shaikh: 3D atom probe, surface reconstruction from image

Mengistu: atmospheric inversion

Meryem: effect of aerosol uncertainty on atmosphere

David Tiego: Path-integral Monte Carlo. aqua-phot
